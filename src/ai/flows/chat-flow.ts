'use server';

/**
 * @fileOverview A simple chat flow that responds to user messages.
 *
 * - chat - A function that handles the chat interaction.
 */

import { ai } from '@/ai/genkit';
import { ChatInputSchema, ChatOutputSchema, type ChatInput } from '@/lib/ai-schemas';
import mammoth from 'mammoth';
import { collection, getDocs, query, orderBy, limit } from "firebase/firestore";
import { db } from '@/lib/firebase';
import {getStorage, ref, getBytes} from 'firebase/storage';

async function getKnowledgeBaseContent(): Promise<string> {
  try {
    const filesCollection = collection(db, "knowledgeBaseFiles");
    const q = query(filesCollection, orderBy("createdAt", "desc"), limit(10)); // Get latest 10 files
    const querySnapshot = await getDocs(q);

    if (querySnapshot.empty) {
      return '';
    }
    
    const storage = getStorage();

    const contents = await Promise.all(
      querySnapshot.docs.map(async (doc) => {
        const fileData = doc.data();
        // The URL in firestore is the download URL, but getBytes expects a storage path.
        // We can create a ref from the download URL itself.
        const fileRef = ref(storage, fileData.url);

        try {
          const bytes = await getBytes(fileRef);
          const buffer = Buffer.from(bytes);

          if (fileData.name.endsWith('.txt')) {
            return buffer.toString('utf-8');
          } else if (fileData.name.endsWith('.docx')) {
            const result = await mammoth.extractRawText({ buffer });
            return result.value;
          }
        } catch (readError) {
          console.error(`Error processing file ${fileData.name} from storage:`, readError);
        }
        return '';
      })
    );

    const nonEmptyContents = contents.filter(content => content && content.trim() !== '');
    if (nonEmptyContents.length > 0) {
      return nonEmptyContents.join('\n\n---\n\n');
    }
  } catch (error) {
    console.error('Error reading knowledge base from Firestore:', error);
  }
  return '';
}

export async function chat(input: ChatInput) {
  return chatFlow(input);
}

const chatFlow = ai.defineFlow(
  {
    name: 'chatFlow',
    inputSchema: ChatInputSchema,
    outputSchema: ChatOutputSchema,
  },
  async ({ history, message, attachment }) => {
    
    const knowledgeBaseContent = await getKnowledgeBaseContent();

    let systemPrompt = `You are TLO, a helpful AI assistant and an expert on the TLOmodel (Thanzi la Onse model).
The TLO Model Framework:
The modelling framework is of a modularised design implemented in Python and numerical libraries. An efficient individual-based simulation engine is used to track a population and the action of ‘events’ that are generated ‘modules’. There are three main types of module:
- Core modules: these represent basic processes, such as, the ‘Demography’ module determines the district of residence of each person, the ‘Lifestyle’ module which determines patterns of risk factors in the population, the ‘Contraception’ module, which represents the contraceptive use of each women, and the ‘HealthSeekingBehaviour’ module which determines if and how persons seek healthcare following onset of an illness.
- The ‘HealthSystem’ module: this represents all functions of the healthcare system – its resources and how these are used to generate effective capabilities, and how these capabilities are distributed among the healthcare needs in the population (generated by the disease modules).
- Disease modules: these represents a specific disease, or set of diseases, including the onset, progression, health outcomes and the effect of any treatment received. If the disease is communicable, transmission is represented within the population. The framework comprises a full grammar of disease module construction and interaction designed such that the access to and effects of treatments are subject to gating/modifying according to resource availability and management decisions in the HealthSystem.

The framework also comprises a suite of utilities that simplify programming and running of these modules; including, the ‘SymptomManager’, which tracks patterns of symptoms in each person, the ‘DxManager’, which represents the usage of diagnostics (and their imperfect performance) and the ‘HealthBurden’ module which tracks the life-years and disability-adjusted life-years in the population.

The model is developed using a system of continuous integration, review, testing and profiling of code. The model runs on cloud computing platforms using a bespoke system of batch-run management.

Your answers should be in-depth, helpful, and based on the provided context. You can format your responses with Markdown.`;

    if (knowledgeBaseContent) {
      systemPrompt += `\n\nYou have also been provided with the following information from documents in a knowledge base. Use this to supplement your knowledge and answer user questions. If the user's question is not covered by the information, state that you do not have information on that topic based on the provided documents.

START OF KNOWLEDGE BASE
${knowledgeBaseContent}
END OF KNOWLEDGE BASE`;
    } else {
      systemPrompt += `\nThe knowledge base is currently empty. Answer questions to the best of your ability based on the framework description provided.`
    }

    const promptParts: any[] = [{ text: message }];

    if (attachment) {
      if (attachment.contentType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
        // It's a .docx file, extract text content.
        const base64Data = attachment.dataUri.split(',')[1];
        const buffer = Buffer.from(base64Data, 'base64');
        const { value: docxText } = await mammoth.extractRawText({ buffer });
        
        // Prepend the document context to the prompt parts array
        promptParts.unshift({
          text: `Please use the following document to answer the user's question. DOCUMENT CONTENT: """${docxText}"""\n\n`,
        });

      } else {
        // For other file types (like images), add the data URI as a media part.
        promptParts.push({ media: { url: attachment.dataUri } });
      }
    }

    const { text } = await ai.generate({
      model: 'googleai/gemini-2.0-flash',
      system: systemPrompt,
      history: history,
      prompt: promptParts,
    });
    
    return { response: text };
  }
);
