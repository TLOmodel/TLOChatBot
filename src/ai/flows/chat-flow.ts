'use server';

/**
 * @fileOverview A simple chat flow that responds to user messages.
 *
 * - chat - A function that handles the chat interaction.
 */

import { ai } from '@/ai/genkit';
import { ChatInputSchema, ChatOutputSchema, type ChatInput } from '@/lib/ai-schemas';
import * as fs from 'fs/promises';
import * as path from 'path';
import mammoth from 'mammoth';

async function getKnowledgeBaseContent(): Promise<string> {
  const kbPath = path.join(process.cwd(), 'src', 'knowledge-base');
  try {
    const files = await fs.readdir(kbPath);
    const contents = await Promise.all(
      files.map(async (file) => {
        const filePath = path.join(kbPath, file);
        try {
          if (file.endsWith('.txt')) {
            return await fs.readFile(filePath, 'utf-8');
          } else if (file.endsWith('.docx')) {
            const result = await mammoth.extractRawText({ path: filePath });
            return result.value;
          }
        } catch (readError) {
          console.error(`Error reading file ${file}:`, readError);
        }
        return '';
      })
    );
    const nonEmptyContents = contents.filter(content => content && content.trim() !== '');
    if (nonEmptyContents.length > 0) {
      return nonEmptyContents.join('\n\n---\n\n');
    }
  } catch (error) {
    if (error instanceof Error && 'code' in error && error.code === 'ENOENT') {
        console.warn('Knowledge base directory not found. Creating it.');
        await fs.mkdir(kbPath, { recursive: true });
    } else {
        console.error('Error reading knowledge base directory:', error);
    }
  }
  return '';
}

export async function chat(input: ChatInput) {
  return chatFlow(input);
}

const chatFlow = ai.defineFlow(
  {
    name: 'chatFlow',
    inputSchema: ChatInputSchema,
    outputSchema: ChatOutputSchema,
  },
  async ({ history, message, attachment }) => {
    
    const knowledgeBaseContent = await getKnowledgeBaseContent();

    let systemPrompt = `You are TLO, a helpful AI assistant and an expert on the TLOmodel (Thanzi la Onse model).
The TLO Model Framework:
The modelling framework is of a modularised design implemented in Python and numerical libraries. An efficient individual-based simulation engine is used to track a population and the action of ‘events’ that are generated ‘modules’. There are three main types of module:
- Core modules: these represent basic processes, such as, the ‘Demography’ module determines the district of residence of each person, the ‘Lifestyle’ module which determines patterns of risk factors in the population, the ‘Contraception’ module, which represents the contraceptive use of each women, and the ‘HealthSeekingBehaviour’ module which determines if and how persons seek healthcare following onset of an illness.
- The ‘HealthSystem’ module: this represents all functions of the healthcare system – its resources and how these are used to generate effective capabilities, and how these capabilities are distributed among the healthcare needs in the population (generated by the disease modules).
- Disease modules: these represents a specific disease, or set of diseases, including the onset, progression, health outcomes and the effect of any treatment received. If the disease is communicable, transmission is represented within the population. The framework comprises a full grammar of disease module construction and interaction designed such that the access to and effects of treatments are subject to gating/modifying according to resource availability and management decisions in the HealthSystem.

The framework also comprises a suite of utilities that simplify programming and running of these modules; including, the ‘SymptomManager’, which tracks patterns of symptoms in each person, the ‘DxManager’, which represents the usage of diagnostics (and their imperfect performance) and the ‘HealthBurden’ module which tracks the life-years and disability-adjusted life-years in the population.

The model is developed using a system of continuous integration, review, testing and profiling of code. The model runs on cloud computing platforms using a bespoke system of batch-run management.

Your answers should be in-depth, helpful, and based on the provided context. You can format your responses with Markdown.`;

    if (knowledgeBaseContent) {
      systemPrompt += `\n\nYou have also been provided with the following information from documents in a knowledge base. Use this to supplement your knowledge and answer user questions. If the user's question is not covered by the information, state that you do not have information on that topic based on the provided documents.

START OF KNOWLEDGE BASE
${knowledgeBaseContent}
END OF KNOWLEDGE BASE`;
    } else {
      systemPrompt += `\nThe knowledge base is currently empty. Answer questions to the best of your ability based on the framework description provided.`
    }

    let promptText = message;
    let media: { url: string } | undefined;

    if (attachment) {
      if (attachment.contentType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
        // It's a .docx file, extract text content.
        const base64Data = attachment.dataUri.split(',')[1];
        const buffer = Buffer.from(base64Data, 'base64');
        const { value: docxText } = await mammoth.extractRawText({ buffer });
        promptText = `Using the following document content, please answer my question.\n\nDOCUMENT CONTENT:\n${docxText}\n\nQUESTION:\n${message}`;
      } else {
        // For other file types (like images), pass the data URI directly.
        media = { url: attachment.dataUri };
      }
    }


    const { text } = await ai.generate({
      model: 'googleai/gemini-2.0-flash',
      system: systemPrompt,
      history: history,
      prompt: {
        text: promptText,
        media: media,
      },
    });
    
    return { response: text };
  }
);
